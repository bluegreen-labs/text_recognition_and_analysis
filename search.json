[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text digitization, recognition and analysis",
    "section": "",
    "text": "Preface\nThese are the materials for the course “Text recognition and analysis, 6-7 Feb. 2025” at the Leibniz-Institut für Europäische Geschichte (IEG), Mainz, and in support of COBECORE project research efforts at the Free University Brussels, Belgium. This book will serve as a reference and as a general introduction for all things Handwritten Text Recognition/Optical Character Recognition (HTR/OCR).\nThis reference gives an overview of the most common tools for historical (handwritten) text recognition, but can be applied elsewhere, too. In addition, I will also briefly discuss the initial digitization and potential citizen science components of such projects, leveraging my experience leading the Congo basin eco-climatological data recovery and valorisation project. It will discuss the practical issues of such projects and how to resolve them efficiently and cost-effectively. This course is a practical tool, not a theoretical machine learning reference. This course will give you an idea of what it takes to start, and complete, a text recognition and analysis effort.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The HTR/OCR workflow\nUnderstanding or translating large volumes of handwritten historical text is critical for historical analysis preservation of text, dissemination of knowledge and valorization of archived measurements and/or other scientific observations. However, reading and processing these large volumes of historical texts (at scale) is often difficult and time consuming. The automation of this process would therefore help in many historical analysis, data recovery and other digital preservation efforts.\nHandwritten text recognition (HTR), contrary to optical character recognition (OCR) for typed texts, is a relatively complex process. Handwritten text (or old fonts) are surprisingly varied, with characters varying from one person (or book) to the next. These variations make HTR/OCR at times an intractable problem.\nGenerally, an HTR/OCR workflow follows two general steps: line/text detection and text transcription. The former detects lines or written text, once detected these lines or text elements are evaluated one-by-one using a text transcription method and combined to form the final digital text document.\nHIGHLIGHT/REFERENCE THE COLOURED BITS IN THE FIGURE\nFigure 1.1: The HTR/OCR workflow, from image acquisition to transcribed HTR/OCR results.\nDepending on the framework or workflow different machine learning (ML) methods of text detection and transcription will be used. It is also key to understand that from a practical computer science perspective the problem of HTR/OCR is solved. Although algorithmic improvements will continue to be developed the current state-of-the-art machine learning (ML) methods perform well for many applications. Most of these algorithms, in the abstract, are relatively easy to understand and with today’s software libraries and platforms even quicker to implement. I will briefly discuss various algorithms in section XYZ. A list of common frameworks and software is given in chapters XYZ.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "digitization.html",
    "href": "digitization.html",
    "title": "2  Digitization",
    "section": "",
    "text": "Although this course focuses on text recognition and analysis it is important to note that digitization, the quality of the images and the consistent collection of meta-data, is key to all subsequent processing. If you start a project where the digitization is not yet completed you should consider the importance of the digitization step within the context of all subsequent post-processing and text recognition workflows.\nThe quality of the collected image data and the availability of meta-data has a profound impact on your workflow. Preemptively addressing image quality and meta-data issues can save significant time and effort, even when taking up some more time in planning and data collection.\nSome general guidelines for digitization therefore include:\n\nensuring a proper digitization setup\n\nhigh quality optics (high f-stop value for sharpness)\nuniform shadowless illumination using multiple lights and ring lights\navoid harsh flash based setups (protecting sensitive manuscripts)\n\nensuring a fixed digitization protocol\n\nfixed sequence of tasks involved\nwell documented\ncollect extensive meta-data when feasible\n\nensuring dynamic back-ups to prevent data loss\n\nFinally, if not within your domain expertise reach out to your local collection managers for support and input on all these aspects.\n\n\n\n\n\n\n\n\nFigure 2.1: The COBECORE digitization station, including a reproduction stand, cold lights, a DSLR camera and a black matte background",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digitization</span>"
    ]
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "3  Computer vision and machine learning",
    "section": "",
    "text": "3.1 Computer vision\nAs highlighted in Figure 2.1, there are two main ML components to HTR/OCR transcription workflows, a segmentation component and a text transcription component. To understand the software (frameworks) for HTR/OCR solutions a brief introduction in ML and computer vision methods is required. This allows you to understand potential pitfalls better.\nAlthough computer vision methods, broadly, include ML methods the classical approaches differ significantly from ML methods. Classic computer vision methods, as discussed below LINK, are applied on pixel (region) or image based transformation. These methods are often used in the pre-processing of images before a machine learning algorithm is applied (FIGURE LINK). Classical examples are the removal of uneven lighting across an image using adaptive histogram equalization, the detection of structuring elements such as linear features using a Hough transform, or the adaptive thresholding of an image from colour to black-and-white only. These algorithms also serve an important role in the creation of additional data from a single reference dataset, through data augmentation LINK.\nFigure 3.1: Example of various thresholding methods as implemented in the OpenCV computer vision library (https://opencv.org)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computer vision and machine learning</span>"
    ]
  },
  {
    "objectID": "ml_methods.html#machine-learning",
    "href": "ml_methods.html#machine-learning",
    "title": "3  Computer vision and machine learning",
    "section": "3.2 Machine Learning",
    "text": "3.2 Machine Learning\nThe machine learning components of the text segmentation and transcriptions rely on common machine learning algorithms and logic. To better understand these tasks, and how training methods influences the success of these models, I will summarize some of these common building blocks. These are vulgarized and simplified descriptions to increase the broad understanding of these processes, for in depth discussions I refer to the linked articles in the text and machine learning textbooks at the end of this course.\n\n\n\n\n\n\n\n\nFigure 3.2: Machine Learning as summarized by XKCD (https://xkcd.com/1838/)\n\n\n\n\n\nMachine learning models are non-deterministic and rely on learning or training (an optimization method) on ground truth (reference) data. The most simple machine learning algorithm is a simple linear regression. In a simple linear regression one optimizes (trains) a slope and intercept parameter to fit the observed response (ground truth) to explanatory variables (data). The more complex the task, the more parameters and data are required. Although oversimplified, the very tongue in cheek cartoon by XKCD is a good mental model of what happens on an abstract level where we shuffle model parameters until we get good correspondence between the data input and the ground truth observations.\nFrom this one can deduce a number of key take-home message:\n\na sufficient amount of training data\nan appropriate ML and shuffling (optimization) algorithm\na ML model is limited by the representations within the training data\n\n\n3.2.1 Detecting patterns: convolutional neural networks (CNN)\nThe analysis of images within the context of machine learning often (but not exclusively) happens using a convolutional neural networks (CNNs). Conceptually a CNN can be see as taking sequential sections of the image and summarizing them (i.e. convolve them) using a function (a filter), to a lower aggregated resolution (FIGURE XYZ). This reduces the size of the image, while at the same time while summarizing a certain characteristic using a filter function. One of the most simple functions would be taking the average value across a 3x3 window.\n\n\n\n\n\n\n\n\nFigure 3.3: An example convolution of a 3x3 window across a larger blue image summarizing values (squares) to a smaller green image (by Kaivan Kamali at https://galaxyproject.org/)\n\n\n\n\n\nIt is important to understand this concept within the context of text recognition and classification tasks in general. It highlights the fact that ML algorithms do not “understand” (handwritten) text. Where people can make sense of handwritten text by understanding the flow, in addition to recognizing patterns, ML approaches focus on patterns, shapes or forms. However, some form of memory can be included using other methods.\n\n\n3.2.2 Memory and context: recurrent neural networks\nA second component to many recognition tasks is a form of memory. Where the CNN encodes for patterns it does so without explicitly taking into account the relative position of these patterns and their relationship to adjacent ones. Here, Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks provide a solution. These algorithms allow for some of the information of adjacent data (either in time or space) to be retained to provide context on the current (time or space) position. Both these approaches can be uni- or bi-directional. In the former, the direction of processing matters in the latter it doesn’t.\n\n\n3.2.3 Negative space: connectionist temporal classification\nIn speech and written text much of the structure is defined not only by what is there, the spoken and written words, but also what is not there, the pauses and spacing. Taken to the extreme the expressionist / dadaist poem “Boem paukeslag” by Paul van Ostaijen is an example of irregularity in typeset text. These irregularities or negative space in the pace of writing is another hurdle for text recognition algorithms. Generally, we want a readable text as output of our ML models not dadaist impressions with large gaps.\n\n\n\n\n\n\n\n\nFigure 3.4: Boem paukenslag by Paul van Ostaijen\n\n\n\n\n\nThese issues in detecting uneven spacing are addressed using the Connectionist Temporal Classification (CTC). This function is applied to the RNN and LSTM output, where it collapses a sequence of recurring labels through oversampling to its most likely reduced form.\n\n\n\n\n\n\n\n\nFigure 3.5: A visualization of the CTC algorithm adapted from Hannun, ‘Sequence Modeling with CTC’, Distill, 2017. doi: 10.23915/distill.00008\n\n\n\n\n\n\n\n3.2.4 Data augmentation\nSufficient data is key in training a ML model which performs well. However, at times you might be limited in the data you can access for training. A common issue is that limited ground truth data (labels, text transcriptions) are available. Data augmentation is a way to slightly alter a smaller existing dataset in order to create a larger, partially, artificial dataset. Within the context of HTR/OCR one can generate slight variations of the same text image and label pair through computer vision (or machine learning) based alterations, such as rotating skewing and introducing noise to the image.\n\n\n\n\n\n\n\n\nFigure 3.6: Data augmentation examples on the French word Juillet",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computer vision and machine learning</span>"
    ]
  },
  {
    "objectID": "ml_methods.html#implementation",
    "href": "ml_methods.html#implementation",
    "title": "3  Computer vision and machine learning",
    "section": "3.3 Implementation",
    "text": "3.3 Implementation\nPutting all the pieces together the most common ML implementation of text segmentation rely heavily on CNN based segmentation networks, while text recognition often if not always takes the form of a CNN + (bidirectional) LSTM/RNN + CTC network. When reading technical documentation on the architecture of models in text transcription frameworks you might come across these terms. Depending on the implementation or framework used data augmentation during training might be provided to increase the scope of the model and increase the chances of Out-Of-Distribution (OOD) generalization.\n\n3.3.1 Out-of-distribution generalization in text transcription\nHandwritten text or old print is highly varying in shape form and retained quality. This pushes trained models towards poor performance as the chances of good OOD generalization are small. In short, two text styles are rarely the same or not similar enough for a trained model to be transferred to a new, seemingly similar, transcription task.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computer vision and machine learning</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "4  HTR/OCR software and data",
    "section": "",
    "text": "4.1 Commercial\nWithin the context of text recognition and analysis there are number of commercial and open-source options available. Below I will list the most common frameworks and some of their advantages and disadvantages. An in-depth discussion on how best to chose a framework within the context of your project is given in the next chapter (REFERENCE).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HTR/OCR software and data</span>"
    ]
  },
  {
    "objectID": "software.html#commercial",
    "href": "software.html#commercial",
    "title": "4  HTR/OCR software and data",
    "section": "",
    "text": "4.1.1 Transkribus\nA dominant player in the transcription of historical texts is the Transkribus platform. This platform provides a way to apply layout detection, text transcription and custom model training (with on platform generated ground truth data) without coding. It offers commercial support options and a growing community of users, including their shared model zoo. The platform is currently built around the PyLaia (python) library (also below).\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\nexpensive\n\n\nsupport / documentation\nvendor lock-in\n\n\nallows custom model training\n\n\n\nmodel sharing\n\n\n\n\n\n\n4.1.2 Google / Amazon / Microsoft APIs\nAll three big tech platforms offer OCR based application programming interfaces (APIs) which you can access from (python) scripts.\nIn particular, HTR/OCR is covered by:\n\nMicrosoft Azure Document Inteligence\nGoogle Vision AI\nAmazon Textract\n\nIncreasingly there is a consolidation of these toolboxes into (multi-modal) Generative Pre-trained Transformer (GPT, as in ChatGPT) based models. These models will provide impressive results on common tasks, but will not perform well on less common or more complex data. Their advantage often lies in their large training corpus.\n\n\n\n\n\n\n\nPro:\nCon:\n\n\n\n\nsupport / documentation\nvendor lock-in\n\n\nscalability\nrequires programming\n\n\nrelatively cheap\ncustom model training is often complex or not possible",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HTR/OCR software and data</span>"
    ]
  },
  {
    "objectID": "software.html#open-source",
    "href": "software.html#open-source",
    "title": "4  HTR/OCR software and data",
    "section": "4.2 Open source",
    "text": "4.2 Open source\n\n4.2.1 eScriptorium\neScriptorium is a software platform created to make text layout analysis and recognition easy. The underlying text recognition is based on the Kraken framework, for which it serves as an interface. The interface allows for the user to annotate and train custom models, with no coding required, similar to Transkribus. Despite providing much the same features as Transkribus, eScriptorium is not program as such, but a service to be run on a server or in a docker image. This does require knowledge on how to setup and manage docker instances, or do a full server install. Good introductions to the use eScriptorium are provided through the standard documentation and a course by the University of Mannheim.\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\ncomplex installation for novices\n\n\nOK documentation\n\n\n\nfull workflow control\n\n\n\ninteroperability\n\n\n\nshared models\n\n\n\n\n\n4.2.1.1 Installation & Use\nA basic docker install is provided on the project code pages.\n\n\n\n4.2.2 OCR4all\nOCR4all is an OCR platform built around the Calamari text recognition engine and the LAREX layout analysis tool. Similar to eScriptorium and Transkribus it aims at making the transcription of documents easy, without the need for coding. Similar to eScriptorium the setup is not program as such, but a service to be run on a server or in a docker image.\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\ncomplex installation for novices\n\n\nOK documentation\n\n\n\nfull workflow control\n\n\n\ninteroperability\n\n\n\nshared models\n\n\n\n\n\n4.2.2.1 Installation & Use\nThe software runs as a docker service and can be installed using the following command:\nsudo docker run -p 1476:8080 \\\n    -u `id -u root`:`id -g $USER` \\\n    --name ocr4all \\\n    -v $PWD/data:/var/ocr4all/data \\\n    -v $PWD/models:/var/ocr4all/models/custom \\\n    -it uniwuezpd/ocr4all\n\n\n\n4.2.3 Tesseract\nTesseract is a popular open-source OCR program, originally created by Google but now maintained by the open-source community. Out of the box Tesseract does not allow for handwritten text recognition as the included models are not trained on handwritten data.\nHowever, the software does allow for the retraining of models. Having been a mainstay in OCR work in the open source community a zoo of third party software providing interfaces and additional functionality exists, as well as a python interface (pytesseract) to make data processing easier.\n\n\n4.2.4 Custom pipelines and libraries\nMost of the above mentioned software options are mature and require limited coding knowledge to operate. However, I would be amiss to not mention the underlying HTR/OCR programming libraries. Depending on the use case one could benefit from using low level libraries, rather than more user friendly platforms (built around them). Most prominent python libraries for HTR/OCR work are Kraken as used by eScriptorium, PyLaia used by Transkribus, EasyOCR and PaddleOCR.\nAll these libraries provide machine learning setups to train handwritten text recognition models of the CNN + LSTM/RNN + CTC kind. In addition, Kraken and PaddleOCR provide document layout analysis (segmentation) options.\n\n\n\nPro:\nCon:\n\n\n\n\nflexible\ncomplex installation\n\n\nfull workflow control\ncoding required",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HTR/OCR software and data</span>"
    ]
  },
  {
    "objectID": "software.html#data",
    "href": "software.html#data",
    "title": "4  HTR/OCR software and data",
    "section": "4.3 Data",
    "text": "4.3 Data\nMethodologically (see above) the problem of text transcription seems to be solved, with various software solutions available. So, what is holding back (universal) open-source HTR/OCR? Generally, data is what holds back HTR in practice.\nGiven the many variations in handwritten text ML algorithms need to be trained (“see”) a wide variety of handwritten text characters to be able to firstly translate similarly styled handwritten text, secondly potentially apply this to other adjacent styles. How close two documents are in writing style determines how well a trained model will perform on this task.\nConsequently, the more variations in handwritten text styles you train an ML algorithm on the easier it will be to transcribe a wide variety of text styles. In short, the bottleneck in automated transcription is gathering sufficient training data (for your use case). Although the ML code might be open-source many large training datasets are not always shared as generously. It can be argued that within the context of FAIR research practices ML code disseminated without the training data, or model parameters, for a particular study is decidedly not open-source. A similar argument has been made within the context of the recent flurry of supposedly open-source Large Language Models (LLMs), such as ChatGPT.\nThe lack of access to both the training data, or a pre-trained model, limits the re-use of the model in a new context. One can not take a model and fine-tune it, i.e. let it “see” new text styles. In short, if you only have the underlying model code you always have to train a model from scratch (anew) using your own, often limited, dataset. This context is important to understand, as this is how transcription platforms will keep you tied to their paying service. It is important to understand these dynamics.\nIncreasingly the need to share data and models openly has come into focus. For example HTR United is an ininiative to collect various HTR/OCR transcription datasets using a common meta-data scheme to break this pattern.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HTR/OCR software and data</span>"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "5  Project management",
    "section": "",
    "text": "5.1 Cost\nI have shown that there is a wide selection of HTR/OCR software to chose from, largely divided between commercial supported and scalable solutions and their open-source alternatives. However, it is important to consider all potential factors in a cost-benefit analysis before settling on a final strategy for data transcription. Below I break down factors influencing cost, and provide a number of strategies for a range of projects (taking into account the cost-benefit factor).\nGenerally two factors should be considered, full cost at scale and interoperability.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "project_management.html#cost",
    "href": "project_management.html#cost",
    "title": "5  Project management",
    "section": "",
    "text": "5.1.1 Scale\nMany transcription projects suffer from the law of large numbers, or in this case death by a thousand paper cuts. In short, data processing at a small cost will blow up to a significant sum given enough data. To illustrate how cost at scale can blow up quickly I will use my own COBECORE project output as an example. In this project ~75K tables (pages) of climate data were digitized and partially transcribed through citizen science.\nWhen considering for example the Transcribus project as a potential option for transcription I calculated that the extraction of tables (1 credit), and its fields (1 credit), and text detection and transcription (1 credit) will require 3 credits per page. For the ~75K tables in the archive this would represent 225K Transkribus credits, with a data volume &gt; 200GB a Team plan is required which would generate a cost of 60 000 EURO. This is under the assumption that no re-runs are required (i.e. a perfect result on a first pass). Experience teaches that ML is often iterative, and the true costs will probably be far higher (easily double the simplest estimate). Various vision APIs of Google or Amazon are cheaper, but don’t allow for easy training and requires coding, in which case you will have to add a (cloud) data engineer/scientist. This shows that when tasks become large, with more complex workflows, alternatives might be cost effective, especially if you are paying a software developer anyway.\n\n\n5.1.2 Interoperability\nInteroperability, or the freedom to use your annotations and models as you like, is an important factor to consider as it influences cost at scale but also the potential to collaborate easily. Most open-source platforms, due to the nature of these projects, share their whole workflow without restrictions. This makes that you can easily share your annotation data for re-use in a colleagues project. Or have your trained model run on a new text of a friend. The same can be said for say Transkribus or the Google Vision API. However, you would force your friend to use a paying service to access your shared data. This lock-in situation often comes at a cost, which does not scale in favour of users and their own contributions. Before settling on a particular software (service) take into account how easy is it to escape the faustian bargain of certain platforms (or APIs) and their vendor lock-in. To quote The Eagle’s Hotel California: “You can check out any time you like, But you can never leave”.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "project_management.html#project-strategies",
    "href": "project_management.html#project-strategies",
    "title": "5  Project management",
    "section": "5.2 Project strategies",
    "text": "5.2 Project strategies\nWith the above cost factors in mind I will sketch what I would see as the optimal strategies in a number of situations. However, some mediating factors might come into play. For example, sponsored deals via universities for various commercial platform (API) solutions are not taken into account. These strategies assume financial independence of the lab/team involved. I also assume that nobody on the team is sufficiently versed in programming from the start, i.e. balancing user-friendliness with cost.\n\n\n\n\n\n\nNote\n\n\n\nThe size of the project (small/large) refers to the size of the data to be processed, not the available funding! I also assume that no digitization is needed, which is expensive slow manual labour.\n\n\n\n5.2.1 Small project, small team\nGenerally, for small projects on a small team I would suggest using Transkribus. Outside learning to use the platform the barrier of entry is low and the cost manageable. This approach would be ideal for a limited corpus of texts within the context of a (Phd/Masters) thesis. Do take into account potential future growth and the limits on interoperability! If a Phd spins off into a larger proposal to process more data costs can grow substantially.\n\n\n5.2.2 Small project, large team\nWhen a team grows so does the likelihood of having someone with tech skills on board. This opens the possibility to consider open-source alternatives, which do require some initial tooling for setup but will perform really well once this part is done. In this use case, I would suggest eScriptorium or OCR4all, in that order.\n\n\n5.2.3 Large project, large team\nWhen a project grows so does the potential for collaborations. As such interoperability becomes an even bigger argument to pick an open-source approach. Similarly, as for smaller projects, I would suggest eScriptorium or OCR4all. Depending on the complexity of the project, which might go up with scale as well, you might benefit from a custom approach built around one of the low level libraries (Kraken, PyLaia, PaddleOCR).\n\n\n5.2.4 Large project, small team\nYou bit of more than you can chew if no matching funding is available. Unless there is a large amount of funding, and a small team can acquire the technological expertise. This is not a favourable position to be in. You will be unlikely to deliver on the promises made in proposals with this dynamic. In this use case the only option left is to either retool quickly and gain (or borrow) the expertise to run an open-source option, such as eScriptorium, or pay for as far as you can get using Transkribus. If, but only if, you have the expertise to run a lean open-source based software solution you could move quickly on large volumes of data cheaply. It goes without saying that it would benefit to build this capacity.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "6  Tips and tricks",
    "section": "",
    "text": "6.1 Capacity building\nAs highlighted in the project management section the scale of your project and the capacity of your team define the optimal approach. However, across the four scenarios provided a common notion is important, capacity building.\nWithin an academic context contracts are often limited in time. In addition, people move frequently between positions search for more stable (research) positions. This presents the danger of knowledge leakage. Preventing this slow trickle of disappearing knowledge requires redundancy in your project management approach, where the responsibility of key transcription components are not the sole responsibility of one person. Generally, this advice does not only apply to transcription projects, but most academic endeavours.\nWhen teaching people transcription workflows, or the setup of a particular piece of software, do so in pairs. In addition, extensively document the process. Although courses and documentation exists for all software discussed these are generalized workflows and do not account for idiosyncrasies within your dataset.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks</span>"
    ]
  },
  {
    "objectID": "tips_tricks.html#text-annotation",
    "href": "tips_tricks.html#text-annotation",
    "title": "6  Tips and tricks",
    "section": "6.2 Text annotation",
    "text": "6.2 Text annotation\nText annotation is a time consuming and often difficult process. It often combines the need for domain knowledge with a very tedious task, as the goal is not to understand the text but to gather data. Dividing this task among trusted authorities such as student contributions might speed up this process. Once, as mentioned above, having a good manual at hand and the capacity within a research team to quickly teach this to someone new is key to the success of such an approach. Regardless, do not underestimate the time you need to invest in this process, so make it worth your while an pick your battles carefully.\n\n6.2.1 Citizen science\nAlternative options to create training data include the use of citizen science platforms, such as Zooniverse (LINK). This is a valid strategy, which also includes an outreach component. However, citizen science is not a way to get “free data”. When done properly there is a good rapport between the community which one engages with and their efforts. Most citizen science efforts will require half an hour of someone’s time to keep going, provide feedback and intermittent results to keep the community motivated. Furthermore, one should use citizen science because it is an easy way to get (training) data while not all other options such as data augmentation on smaller datasets have been exhausted. The latter approaches are often required regardless of data size. Assessing the accuracy of suite of models is required before concluding that more training data is needed, than can reasonably be generated within a team.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks</span>"
    ]
  },
  {
    "objectID": "tips_tricks.html#data-augmentation",
    "href": "tips_tricks.html#data-augmentation",
    "title": "6  Tips and tricks",
    "section": "6.3 Data augmentation",
    "text": "6.3 Data augmentation\nDo not underestimate the power of image augmentation to make your HTR/OCR algorithm more robust. Where you have control over this process it is advisable to use it. In this context, it is also generally a poor idea to apply extensive computer vision based pre-processing to the text, outside proper cropping or aligning of the pages. Especially, binarization of images (converting from RGB to black and white only) can produce unstable HTR output, as the pre-processing step has an outside influence as it is linked to data loss. Conversely, image augmentation by introducing noise can make results more robust to these disturbances, real or not.\n\n\n\n\n\n\n\n\nFigure 6.1: Data augmentation examples on the French word Juillet, including a binarization bottom left.\n\n\n\n\n\n\n6.3.1 Synthetic data\nTaking image augmentation to the extreme is the creation of synthetic data. Here, you don not transform original data but create a fully artificial dataset. This approach often requires custom scripts, but might be a way to sidestep the annotation of texts.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tips and tricks</span>"
    ]
  }
]