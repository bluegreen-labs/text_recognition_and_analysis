[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text digitization, recognition and analysis",
    "section": "",
    "text": "Preface\nThese are the materials for the course “Text recognition and analysis, 6-7 Feb. 2025” at the Leibniz-Institut für Europäische Geschichte (IEG), Mainz, with lessons learned from from the extension of the Congo basin eco-climatological data recovery and valorisation (COBECORE) project research efforts with the Free University Brussels, Belgium. This document will serve as a (personal) reference and as a general introduction for all things Handwritten Text Recognition/Optical Character Recognition (HTR/OCR).\nThis reference gives an overview of the most common tools and (data) pitfalls for historical (handwritten) text recognition, but it can be applied elsewhere, too. In addition, I will also briefly discuss the initial digitization and potential citizen science components of such projects, leveraging my experience leading the COBECORE project. I will discuss the practical issues of text recognition projects and how to resolve them efficiently and cost-effectively. This reference is a practical tool, not an introduction to machine learning. This reference will give you guidance on what it takes to start, and complete, a text recognition and analysis effort.\n\n\n\n\n\n\nNote\n\n\n\nThis is not a machine learning reference! Drastic simplifications are made, using analogies, for the sake of clarity due to the interdisciplinary nature of transcription projects. Computer and data science majors might find these references “wrong”. However, the goals is not to be mathematically correct, but to communicate the processes of a text recognition project broadly.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The HTR/OCR workflow\nUnderstanding or translating large volumes of handwritten historical text is critical for historical analysis preservation of text, dissemination of knowledge and valorization of archived measurements and/or other scientific observations. However, reading and processing these large volumes of historical texts (at scale) is often difficult and time consuming. The automation of this process would therefore help in many historical analysis, data recovery and other digital preservation efforts.\nHandwritten text recognition (HTR), contrary to optical character recognition (OCR) for typed texts, is a relatively complex process. Handwritten text (or old fonts) are surprisingly varied, with characters varying from one person (or book) to the next. These variations make HTR/OCR at times an intractable problem.\nGenerally, an HTR/OCR workflow follows two general steps: line/text detection and text transcription. The former detects lines or written text, once detected these lines or text elements are evaluated one-by-one using a text transcription method and combined to form the final digital text document.\nFigure 1.1: The HTR/OCR workflow, from image acquisition to transcribed HTR/OCR results. Light yellow highlights the image acquisition and pre-processing (see Chapter 2 and 3), blue and green areas deal with the machine learning processing (Chapter 3 and 4). The green area deals with the training of machine learning models discussed in Chapter 4 and 5.\nDepending on the framework or workflow different machine learning (ML) methods of text detection and transcription will be used. It is also key to understand that from a practical computer science perspective the problem of HTR/OCR is solved. Although algorithmic improvements will continue to be developed the current state-of-the-art machine learning (ML) methods perform well for many applications. Most of these algorithms, in the abstract, are relatively easy to understand and with today’s software libraries and platforms even quicker to implement. I will briefly discuss various algorithms in Chapter 4. A list of common frameworks and software is given in Chapter 6 .",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "digitization.html",
    "href": "digitization.html",
    "title": "2  Digitization",
    "section": "",
    "text": "2.1 Basic digitization equipment\nAlthough this reference manual focuses on text recognition and analysis it is important to note that digitization, and the quality of the images and the consistent collection of meta-data, is key to all subsequent processing. If you start a project where the digitization is not yet completed you should consider the importance of the digitization step within the context of all subsequent post-processing and text recognition workflows.\nThe quality of the collected image data and the availability of meta-data has a profound impact on your workflow. Preemptively addressing image quality and meta-data issues can save significant time and effort, even when taking up some more time in planning and data collection.\nCommon issues are the introduction of noise and lack of contrast of the text due to bleed through of the structure of the paper, or data on the other side of a page. Within the context of COBECORE a quick comparison between using either a black or white matte background showed a large jump in the noise level for a black matte. A white matte background was used in the final digitization to boost contrast.\nA basic digitization setup consists of:\nThe most basic setup will cost you less than 1500 EURO. Using a good reproduction table and cold lights will cost more, but does not necessarily yield better results.\nFigure 2.2: The COBECORE digitization station, including a reproduction stand, cold lights and ring light, a DSLR camera, a black matte background around the document, and a laptop computer with external hard drive for storage and backups.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digitization</span>"
    ]
  },
  {
    "objectID": "digitization.html#basic-digitization-equipment",
    "href": "digitization.html#basic-digitization-equipment",
    "title": "2  Digitization",
    "section": "",
    "text": "a digital camera (DSLR or mirrorless equivalent) and high quality optics (~1000 EURO)\na tripod with a horizontal boom or a reproduction table (~150 EURO)\na (ring) light setup or plenty of available (natural) light casting no shadows, do not use flash lights (~50 EURO)\na background matte (&lt;10 EURO)\na fixation option to keep data records in place (think of magnets, binder klips etc.) (&lt;10 EURO)",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digitization</span>"
    ]
  },
  {
    "objectID": "digitization.html#general-guidelines",
    "href": "digitization.html#general-guidelines",
    "title": "2  Digitization",
    "section": "2.2 General guidelines",
    "text": "2.2 General guidelines\nIn addition to the physical setup of the camera and lights, you will need a computer with sufficient storage capacity and a (institutional) backup solution. To ensure consistent digitization a protocol should be written to detail a fixed sequence of tasks, including the collection of meta-data. Finally, if these aspects are not within your domain of expertise reach out to your local collection managers for support and input!",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digitization</span>"
    ]
  },
  {
    "objectID": "pre-processing.html",
    "href": "pre-processing.html",
    "title": "3  Pre-processing",
    "section": "",
    "text": "3.1 Computer vision\nThere are two main ML components to HTR/OCR transcription workflows (Figure 1.1), a segmentation component and a text transcription component. To understand the software (frameworks) for HTR/OCR solutions a brief introduction in ML and computer vision pre-processing methods is required. This allows you to understand potential pitfalls better.\nAlthough computer vision methods, broadly, include ML methods the classical approaches differ significantly from ML methods. Classic computer vision methods are applied on pixel (region) or image based transformation. These methods are often used in the pre-processing of images before a machine learning algorithm is applied Figure 1.1. In particular, the removal of noise, boosting of text contrast and creation of evenly lighted documents are common pre-processing steps.\nThese algorithms also serve an important role in the creation of additional (synthetic) data from a single reference dataset, through data augmentation Figure 5.1, in order to increase a machine learning model robustness.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pre-processing</span>"
    ]
  },
  {
    "objectID": "pre-processing.html#key-pre-processing-concepts",
    "href": "pre-processing.html#key-pre-processing-concepts",
    "title": "3  Pre-processing",
    "section": "3.2 Key pre-processing concepts",
    "text": "3.2 Key pre-processing concepts\nClassical examples are the removal of uneven lighting across an image using (contrast limited) adaptive histogram equalization (CLAHE), the detection and removal of structuring elements such as linear features using a Hough transform, or the adaptive thresholding of an image from colour to black-and-white only.\n\n\n\n\n\n\n\n\nFigure 3.1: Example of various thresholding methods as implemented in the OpenCV computer vision library (https://opencv.org)\n\n\n\n\n\nOther common methods are the use of Non-Local Means de-noising to remove stochastic noise. Other Fast Fourier Transform (FFT) based methods can be applied to remove periodic noise by manipulating the frequency domain of the image.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pre-processing</span>"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "4  Machine Learning",
    "section": "",
    "text": "4.0.1 Detecting patterns: convolutional neural networks (CNN)\nThe machine learning components of the text segmentation and transcriptions rely on common machine learning algorithms and logic. To better understand these tasks, and how training methods influences the success of these models, I will summarize some of these common building blocks. These are vulgarized and simplified descriptions to increase the broad understanding of these processes, for in depth discussions I refer to the linked articles in the text and machine learning textbooks at the end of this course.\nMachine learning models are non-deterministic and rely on learning or training (an optimization method) on ground truth (reference) data. The most simple machine learning algorithm is a simple linear regression. In a simple linear regression one optimizes (trains) a slope and intercept parameter to fit the observed response (ground truth) to explanatory variables (data). For more complex tasks, with more parameters, more data are required. Although oversimplified, the very tongue in cheek cartoon by XKCD (Figure 4.1) is a good mental model of what happens on an abstract level where we shuffle model parameters until we get good correspondence between the data input and the ground truth observations.\nFrom this one can deduce a number of key take-home message:\nThe analysis of images within the context of machine learning often (but not exclusively) happens using a convolutional neural networks (CNNs). Conceptually a CNN can be seen as taking sequential sections of the image and summarizing them (i.e. convolve them) using a function (a filter), to a lower aggregated resolution (Figure 4.2). This reduces the size of the image, while at the same time while summarizing a certain characteristic, using a filter function. One of the most simple functions would be taking the average value across a 3x3 window.\nFigure 4.2: An example convolution of a 3x3 window across a larger blue image summarizing values (squares) to a smaller green image (by Kaivan Kamali at https://galaxyproject.org/)\nIt is important to understand this concept within the context of text recognition and classification tasks in general. It highlights the fact that ML algorithms do not “understand” (handwritten) text. Where people can make sense of handwritten text by understanding the flow, in addition to recognizing broader patterns, ML approaches focus on patterns, shapes or forms. However, some form of memory can be included using other methods.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#the-cnnlstmctc-implementation",
    "href": "machine_learning.html#the-cnnlstmctc-implementation",
    "title": "4  Machine Learning",
    "section": "4.1 The CNN+LSTM+CTC implementation",
    "text": "4.1 The CNN+LSTM+CTC implementation\nPutting all the pieces together the most common ML implementation of text segmentation rely heavily on CNN based segmentation networks, while text recognition often if not always takes the form of a CNN + (bidirectional) LSTM/RNN + CTC network. When reading technical documentation on the architecture of models in text transcription frameworks you might come across these terms. Depending on the implementation or framework used data augmentation (see next section) during training might be provided to increase the scope of the model and increase the chances of Out-Of-Distribution (OOD) generalization.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#transformers",
    "href": "machine_learning.html#transformers",
    "title": "4  Machine Learning",
    "section": "4.2 Transformers",
    "text": "4.2 Transformers\nTransformer based OCR (TrOCR) is new breed of HTR algorithms based on the transformer architecture, providing an alternative to the CNN+LSTM+CTT workflow. Unlike the CNN+LSTM+CTC approach TrOCR does not use convolution but chops up an image context window into adjacent subsets combined with a positional (spatio-temporal) marker. Equivalent to the convolutional setup it retains feature and spatio-temporal marker in decoding a sequence.\n\n4.2.0.1 multi-modal LLM\nMulti-modal large language models such as ChatGPT leverage the Transformer architecture and use them on an incredibly large scale, using mostly text, but also annotated image data. This allows these LLM (chatbots) to make sense of images of text.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "5  Data (generation)",
    "section": "",
    "text": "5.1 Annotating\nMethodologically (see Chapter 4 and Chapter 6) the problem of text transcription seems to be solved, with various software solutions available. So, what is holding back (universal) open-source HTR/OCR? Generally, data is what holds back HTR in practice.\nGiven the many variations in handwritten text, machine learning algorithms need to be trained (“see”) a wide variety of handwritten text characters to be able to translate similarly styled handwritten text, and potentially apply this to other adjacent styles. How close two documents are in writing style determines how well a trained model will perform on this task.\nTo use the XKCD cartoon (Figure 4.1), your (input) data does not resemble the pile used to train the model. Handwritten text or old print is highly varying in shape form and retained quality. This pushes trained models towards poor performance as the chances of good Out-of-Distribution (OOD) generalization are small. In short, two text styles are rarely the same or not similar enough for a trained model to be transferred to a new, seemingly similar, transcription task. Consequently, the more variations in handwritten text styles you train a machine learning algorithm on the easier it will be to transcribe a wide variety of text styles. In short, the bottleneck in automated transcription is gathering sufficient training data (for your use case).\nAlthough the ML code might be open-source many large training datasets are not always shared as generously. It can be argued that within the context of FAIR research practices machine learning code disseminated without the training data, or model parameters, for a particular study is decidedly not open science. A similar argument has been made within the context of the recent flurry of supposedly open-source Large Language Models (LLMs), such as ChatGPT.\nThe lack of access to both the training data, or a pre-trained model, limits the re-use of the model in a new context. One cannot take a model and fine-tune it, i.e. let it “see” new text styles to learn from. In short, if you only have the underlying model code you always have to train a model from scratch (anew) using your own, often limited, dataset. This context is important to understand, as this is how transcription platforms will keep you tied to their paying service. Increasingly the need to share data and models openly has come into focus. For example HTR United is an ininiative to collect various HTR/OCR transcription datasets using a common meta-data scheme to break this pattern.\nOne of the ways to fix OOD issues of machine learning models is to annotate representative data and retrain your model using these data. Conceptually this annotation process is easy. However, text annotation is a time consuming and often difficult process. It combines the need for domain knowledge (skills such as reading of cursive are becoming rare) with a very tedious task. The goal is not to understand the text but to merely gather training data. Some of the software solutions (See Chapter 6) such as eScriptorium, Transkribus, OCR4all provide annotation abilities within their platform. Other options are using open source annotation software/platforms such as CVAT.ai.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data (generation)</span>"
    ]
  },
  {
    "objectID": "data.html#sec-annotating",
    "href": "data.html#sec-annotating",
    "title": "5  Data (generation)",
    "section": "",
    "text": "5.1.1 Citizen science\nAlternative options to create training data include the use of community/citizen science platforms, such as Zooniverse. Here, the annotation of texts is outsourced to volunteers. This works well assuming that the text to be transcribed has a sizable user base. A language which is only written by a small number of people is less likely to be easily transcribed, as many replicates are required for quality control. It must be noted that community/citizen science is not a way to get “free data”. When done properly there should be a good rapport between the community which one engages, and the scientists executing the research.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data (generation)</span>"
    ]
  },
  {
    "objectID": "data.html#data-augmentation",
    "href": "data.html#data-augmentation",
    "title": "5  Data (generation)",
    "section": "5.2 Data augmentation",
    "text": "5.2 Data augmentation\nSufficient data is key in training a ML model which performs well. However, at times you might be limited in the data you can access for training, even after gathering additional annotations. Data augmentation is a way to slightly alter a smaller existing dataset in order to create a larger, partially, artificial dataset. Within the context of HTR/OCR one can generate slight variations of the same text image and label pair through computer vision (or machine learning) based alterations, such as rotating skewing and introducing noise to the image.\n\n\n\n\n\n\n\n\nFigure 5.1: Data augmentation examples on the French word Juillet\n\n\n\n\n\nDo not underestimate the power of image augmentation to make your HTR/OCR algorithm more robust. Where you have control over this process it is advisable to use it (in some of the platforms it is automatically integrated in model training steps). In this context, it is also generally a poor idea to apply extensive computer vision based pre-processing (see Chapter 3 ) to the text, outside proper cropping or aligning of the pages. Especially, binarization of images (converting from RGB to black and white only) can produce unstable HTR output, as the pre-processing step has an outside influence as it is linked to data loss. Conversely, image augmentation by introducing noise can make results more robust to these disturbances, real or not.\n\n5.2.1 Synthetic data\nTaking image augmentation to the extreme is the creation of synthetic data. Here, you don not transform original data but create a fully artificial dataset. This approach often requires custom scripts, but might be a way to sidestep the annotation of texts.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data (generation)</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "6  HTR/OCR software",
    "section": "",
    "text": "6.1 Commercial\nWithin the context of text recognition and analysis there are number of commercial and open-source options available. Below I will list the most common frameworks and some of their advantages and disadvantages. An in-depth discussion on how best to chose a framework within the context of your project is given in the next chapter (REFERENCE).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>HTR/OCR software</span>"
    ]
  },
  {
    "objectID": "software.html#commercial",
    "href": "software.html#commercial",
    "title": "6  HTR/OCR software",
    "section": "",
    "text": "6.1.1 Transkribus\nA dominant player in the transcription of historical texts is the Transkribus platform. This platform provides a way to apply layout detection, text transcription and custom model training (with on platform generated ground truth data) without coding. It offers commercial support options and a growing community of users, including their shared model zoo. The platform is currently built around the PyLaia (python) library (also below).\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\nexpensive\n\n\nsupport / documentation\nvendor lock-in\n\n\nallows custom model training\n\n\n\nmodel sharing\n\n\n\n\n\n\n6.1.2 Google / Amazon / Microsoft APIs\nAll three big tech platforms offer OCR based application programming interfaces (APIs) which you can access from (python) scripts.\nIn particular, HTR/OCR is covered by:\n\nMicrosoft Azure Document Inteligence\nGoogle Vision AI\nAmazon Textract\n\nIncreasingly there is a consolidation of these toolboxes into (multi-modal) Generative Pre-trained Transformer (GPT, as in ChatGPT) based models. These models will provide impressive results on common tasks, but will not perform well on less common or more complex data. Their advantage often lies in their large training corpus.\n\n\n\n\n\n\n\nPro:\nCon:\n\n\n\n\nsupport / documentation\nvendor lock-in\n\n\nscalability\nrequires programming\n\n\nrelatively cheap\ncustom model training is often complex or not possible",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>HTR/OCR software</span>"
    ]
  },
  {
    "objectID": "software.html#open-source",
    "href": "software.html#open-source",
    "title": "6  HTR/OCR software",
    "section": "6.2 Open source",
    "text": "6.2 Open source\n\n6.2.1 eScriptorium\neScriptorium is a software platform created to make text layout analysis and recognition easy. The underlying text recognition is based on the Kraken framework, for which it serves as an interface. The interface allows for the user to annotate and train custom models, with no coding required, similar to Transkribus. Despite providing much the same features as Transkribus, eScriptorium is not program as such, but a service to be run on a server or in a docker image. This does require knowledge on how to setup and manage docker instances, or do a full server install. Good introductions to the use eScriptorium are provided through the standard documentation and a course by the University of Mannheim.\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\ncomplex installation for novices\n\n\nOK documentation\n\n\n\nfull workflow control\n\n\n\ninteroperability\n\n\n\nshared models\n\n\n\n\n\n6.2.1.1 Installation & Use\nA basic docker install is provided on the project code pages.\n\n\n\n6.2.2 ArkIndex\nArkIndex is a document processing platform similar to Transkribus. More so, the this open-source platform is made by the company, Teklia, behind the PyLaia library underpinning most of Transkribus. In therefore offers the same functionality with a different interface.\n\n6.2.2.1 Installation & Use\nA basic docker install is provided on the project documentation pages.\n\n\n\n6.2.3 OCR4all\nOCR4all is an OCR platform built around the Calamari text recognition engine and the LAREX layout analysis tool. Similar to eScriptorium and Transkribus it aims at making the transcription of documents easy, without the need for coding. Similar to eScriptorium the setup is not program as such, but a service to be run on a server or in a docker image.\n\n\n\nPro:\nCon:\n\n\n\n\nuser friendly\ncomplex installation for novices\n\n\nOK documentation\n\n\n\nfull workflow control\n\n\n\ninteroperability\n\n\n\nshared models\n\n\n\n\n\n6.2.3.1 Installation & Use\nThe software runs as a docker service and can be installed using the following command:\nsudo docker run -p 1476:8080 \\\n    -u `id -u root`:`id -g $USER` \\\n    --name ocr4all \\\n    -v $PWD/data:/var/ocr4all/data \\\n    -v $PWD/models:/var/ocr4all/models/custom \\\n    -it uniwuezpd/ocr4all\n\n\n\n6.2.4 Tesseract\nTesseract is a popular open-source OCR program, originally created by Google but now maintained by the open-source community. Out of the box Tesseract does not allow for handwritten text recognition as the included models are not trained on handwritten data.\nHowever, the software does allow for the retraining of models. Having been a mainstay in OCR work in the open source community a zoo of third party software providing interfaces and additional functionality exists, as well as a python interface (pytesseract) to make data processing easier.\n\n\n6.2.5 Custom pipelines and libraries\nMost of the above mentioned software options are mature and require limited coding knowledge to operate. However, I would be amiss to not mention the underlying HTR/OCR programming libraries. Depending on the use case one could benefit from using low level libraries, rather than more user friendly platforms (built around them). Most prominent python libraries for HTR/OCR work are Kraken as used by eScriptorium, PyLaia used by Transkribus, EasyOCR and PaddleOCR. Other software libraries to mention are YOLO and doc-UFCN which both cover layout and text detection needs.\nAll these libraries provide machine learning setups to train handwritten text recognition models of the CNN + LSTM/RNN + CTC kind. In addition, Kraken and PaddleOCR provide document layout analysis (segmentation) options.\n\n\n\nPro:\nCon:\n\n\n\n\nflexible\ncomplex installation\n\n\nfull workflow control\ncoding required",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>HTR/OCR software</span>"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "7  Project management",
    "section": "",
    "text": "7.1 Cost\nI have shown that there is a wide selection of HTR/OCR software to chose from, largely divided between commercial supported and scalable solutions and their open-source alternatives. However, it is important to consider all potential factors in a cost-benefit analysis before settling on a final strategy for data transcription. Below I break down factors influencing cost, and provide a number of strategies for a range of projects (taking into account the cost-benefit factor).\nGenerally two factors should be considered, full cost at scale and interoperability.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "project_management.html#cost",
    "href": "project_management.html#cost",
    "title": "7  Project management",
    "section": "",
    "text": "7.1.1 Scale\nMany transcription projects suffer from the law of large numbers, or in this case death by a thousand paper cuts. In short, data processing at a small cost will blow up to a significant sum given enough data. To illustrate how cost at scale can blow up quickly I will use my own COBECORE project output as an example. In this project ~75K tables (pages) of climate data were digitized and partially transcribed through citizen science.\nWhen considering for example the Transcribus project as a potential option for transcription I calculated that the extraction of tables (1 credit), and its fields (1 credit), and text detection and transcription (1 credit) will require 3 credits per page. For the ~75K tables in the archive this would represent 225K Transkribus credits, with a data volume &gt; 200GB a Team plan is required which would generate a cost of 60 000 EURO. This is under the assumption that no re-runs are required (i.e. a perfect result on a first pass). Experience teaches that ML is often iterative, and the true costs will probably be far higher (easily double the simplest estimate). Various vision APIs of Google or Amazon are cheaper, but don’t allow for easy training and requires coding, in which case you will have to add a (cloud) data engineer/scientist. This shows that when tasks become large, with more complex workflows, alternatives might be cost effective, especially if you are paying a software developer anyway.\n\n\n7.1.2 Interoperability\nInteroperability, or the freedom to use your annotations and models as you like, is an important factor to consider as it influences cost at scale but also the potential to collaborate easily. Most open-source platforms, due to the nature of these projects, share their whole workflow without restrictions. This makes that you can easily share your annotation data for re-use in a colleagues project. Or have your trained model run on a new text of a friend. The same can be said for say Transkribus or the Google Vision API. However, you would force your friend to use a paying service to access your shared data. This lock-in situation often comes at a cost, which does not scale in favour of users and their own contributions. Before settling on a particular software (service) take into account how easy is it to escape the faustian bargain of certain platforms (or APIs) and their vendor lock-in. To quote The Eagle’s Hotel California: “You can check out any time you like, But you can never leave”.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "project_management.html#project-strategies",
    "href": "project_management.html#project-strategies",
    "title": "7  Project management",
    "section": "7.2 Project strategies",
    "text": "7.2 Project strategies\nWith the above cost factors in mind I will sketch what I would see as the optimal strategies in a number of situations. However, some mediating factors might come into play. For example, sponsored deals via universities for various commercial platform (API) solutions are not taken into account. These strategies assume financial independence of the lab/team involved. I also assume that nobody on the team is sufficiently versed in programming from the start, i.e. balancing user-friendliness with cost.\n\n\n\n\n\n\nNote\n\n\n\nThe size of the project (small/large) refers to the size of the data to be processed, not the available funding! I also assume that no digitization is needed, which is expensive slow manual labour.\n\n\n\n7.2.1 Small project, small team\nGenerally, for small projects on a small team I would suggest using Transkribus. Outside learning to use the platform the barrier of entry is low and the cost manageable. This approach would be ideal for a limited corpus of texts within the context of a (Phd/Masters) thesis. Do take into account potential future growth and the limits on interoperability! If a Phd spins off into a larger proposal to process more data costs can grow substantially.\n\n\n7.2.2 Small project, large team\nWhen a team grows so does the likelihood of having someone with tech skills on board. This opens the possibility to consider open-source alternatives, which do require some initial tooling for setup but will perform really well once this part is done. In this use case, I would suggest eScriptorium or OCR4all, in that order.\n\n\n7.2.3 Large project, large team\nWhen a project grows so does the potential for collaborations. As such interoperability becomes an even bigger argument to pick an open-source approach. Similarly, as for smaller projects, I would suggest eScriptorium or OCR4all. Depending on the complexity of the project, which might go up with scale as well, you might benefit from a custom approach built around one of the low level libraries (Kraken, PyLaia, PaddleOCR).\n\n\n7.2.4 Large project, small team\nYou bit of more than you can chew if no matching funding is available. Unless there is a large amount of funding, and a small team can acquire the technological expertise. This is not a favourable position to be in. You will be unlikely to deliver on the promises made in proposals with this dynamic. In this use case the only option left is to either retool quickly and gain (or borrow) the expertise to run an open-source option, such as eScriptorium, or pay for as far as you can get using Transkribus. If, but only if, you have the expertise to run a lean open-source based software solution you could move quickly on large volumes of data cheaply. It goes without saying that it would benefit to build this capacity.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "8  Tips and tricks",
    "section": "",
    "text": "8.1 Mentoring\nAs highlighted in the project management section the scale of your project and the capacity of your team define the optimal approach. However, across the four scenarios provided a common notion is important, capacity building for the longevity of projects.\nWithin an academic context contracts are often limited in time, funds scarce. In addition, people move frequently between positions search for more stable or better paid (research) positions. This presents the danger of knowledge leakage. Preventing this slow trickle of disappearing knowledge requires redundancy in your project management approach, where the responsibility of key transcription components are not the sole responsibility of one person. Generally, this advice does not only apply to transcription projects, but most academic endeavours.\nWhen teaching people transcription workflows, or the setup of a particular piece of software, do so in pairs. In addition, extensively document the process. Although courses and documentation exists for all software discussed these are generalized workflows and do not account for idiosyncrasies within your dataset.\nWhen using annotation (Section 5.1) to provide tailored training data divide this task among trusted authorities, such as students, might speed up this process. Having a good manual at hand and the capacity within a research team to quickly teach this to someone new is key to the success of such an approach. Regardless, do not underestimate the time you need to invest in this process, so make it worth your while an pick your battles carefully.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tips and tricks</span>"
    ]
  },
  {
    "objectID": "tips_tricks.html#communitycitizen-science",
    "href": "tips_tricks.html#communitycitizen-science",
    "title": "8  Tips and tricks",
    "section": "8.2 Community/Citizen science",
    "text": "8.2 Community/Citizen science\nMost community/citizen science efforts will require half an hour of someone’s time to keep going, provide feedback and intermittent results to keep the community motivated. Furthermore, one should use citizen science because it is an easy way to get (training) data while not all other options such as data augmentation on smaller datasets have been exhausted. The latter approaches are often required regardless of data size. Assessing the accuracy of suite of models is required before concluding that more training data is needed than can reasonably be generated within a team.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tips and tricks</span>"
    ]
  }
]